# ğŸ  Housing Regression Machine Learning System

An end-to-end Machine Learning project to predict US housing prices. This system takes raw data, processes it, trains an XGBoost model, and serves predictions via a **FastAPI** backend and **Streamlit** frontend, deployed on **AWS**.

---

## ğŸš€ Features
- **Data Pipeline**: Cleaning, feature engineering, and splitting (Train/Eval/Holdout).
- **Training**: XGBoost with Hyperparameter Tuning (Optuna) and Experiment Tracking (MLflow).
- **Inference**: Production-ready inference pipeline.
- **Web App**: Interactive Streamlit dashboard for exploring predictions.
- **API**: REST API for real-time and batch predictions.
- **Infrastructure**: Dockerized services, deployed on AWS ECS Fargate.
- **CI/CD**: GitHub Actions for automated testing and deployment.

---

## ğŸ› ï¸ Local Setup

### 1. Prerequisites
- Python 3.11+
- [UV](https://github.com/astral-sh/uv) (Recommended) or Pip
- Docker (Optional, for container testing)

### 2. Installation
Clone the repository and install dependencies:
```bash
git clone https://github.com/your-username/housing-regression-ml.git
cd housing-regression-ml

# Using UV (Recommended)
uv sync

# OR using Pip
pip install -r pyproject.toml
```

### 3. Data Setup
1. Download the data from the sources mentioned in the transcript/docs:
   - `House_TS.csv` (Main Data)
   - `us-en-metros.csv` (Geo Data)
2. Place them in `data/row/`:
   ```text
   data/
   â””â”€â”€ row/
       â”œâ”€â”€ House_TS.csv
       â””â”€â”€ us-en-metros.csv
   ```
3. Run the processing pipeline:
   ```bash
   # Example: Run the feature pipeline script (if available) or notebooks
   python src/feature_pipeline/run.py
   ```

---

## â˜ï¸ AWS Deployment Guide

This project is designed to be deployed on AWS using ECS Fargate. Follow these steps to set up your cloud environment.

### 1. AWS Cloud Prerequisites
- **AWS Account**: Ensure you have an active account.
- **IAM User**: Create a user (e.g., `ml-deployer`) with the following permissions:
  - `AmazonS3FullAccess`
  - `AmazonEC2ContainerRegistryFullAccess` (ECR)
  - `AmazonECS_FullAccess`
  - `ElasticLoadBalancingFullAccess`
- **AWS CLI**: Install and configure locally:
  ```bash
  aws configure
  # Enter Access Key, Secret Key, Region (e.g., us-east-1)
  ```

### 2. S3 Bucket Setup
Create an S3 bucket to store models and processed data:
```bash
aws s3 mb s3://housing-ml-artifacts-yourname
```
Update your `.env` or config:
```ini
S3_BUCKET=housing-ml-artifacts-yourname
AWS_REGION=us-east-1
```

### 3. Elastic Container Registry (ECR)
Create repositories for the API and UI images:
```bash
aws ecr create-repository --repository-name housing-regression-api
aws ecr create-repository --repository-name housing-regression-ui
```

### 4. GitHub Actions Secrets
Go to your GitHub Repo -> Settings -> Secrets and Variables -> Actions -> **New Repository Secret**:
- `AWS_ACCESS_KEY_ID`: Your IAM User Key
- `AWS_SECRET_ACCESS_KEY`: Your IAM User Secret
- `AWS_REGION`: e.g., `us-east-1`
- `ECR_REPOSITORY_API`: `housing-regression-api`
- `ECR_REPOSITORY_UI`: `housing-regression-ui`

### 5. Deployment (ECS)
The deployment is handled by GitHub Actions (`.github/workflows/ci-cd.yml`).
1. **Push to Main**: When you push changes to the `main` branch, the workflow will:
   - Run tests.
   - Build Docker images.
   - Push images to ECR.
   - Update the ECS Service (if configured).

> **Note**: For the first deployment, you may need to manually create the **ECS Cluster** and **Service** (Fargate) in the AWS Console and link them to the Task Definitions generated by the workflow.

---

## ğŸ–¥ï¸ Usage

### Running Locally
**API Service**:
```bash
uvicorn src.api.main:app --reload
```
Visit `http://localhost:8000/docs` to test endpoints.

**Streamlit App**:
```bash
streamlit run app.py
```
Visit `http://localhost:8501`.

---

## ğŸ§ª Testing
Run the test suite:
```bash
pytest tests/
```
